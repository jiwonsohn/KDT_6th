{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자연어 처리를 위한 전처리   \n",
    "- 정제  \n",
    "    * 토큰화 전후로 진행됨  \n",
    "      \n",
    "    * 100% 제거는 어려움 !  \n",
    "\n",
    "    * 불용어, 빈도에 따른 제어, 길이에 따른 제거  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모듈로딩\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 불용어 : 179개\n",
      "i\n",
      "me my myself we our ours ourselves you you're you've\n",
      "you'll you'd your yours yourself yourselves he him his himself\n",
      "she she's her hers herself it it's its itself they\n",
      "them their theirs themselves what which who whom this that\n",
      "that'll these those am is are was were be been\n",
      "being have has had having do does did doing a\n",
      "an the and but if or because as until while\n",
      "of at by for with about against between into through\n",
      "during before after above below to from up down in\n",
      "out on off over under again further then once "
     ]
    }
   ],
   "source": [
    "# NLTK 제공 영어 불용어 리스트 가져오기\n",
    "english_sw = stopwords.words('english')\n",
    "\n",
    "print(f'영어 불용어 : {len(english_sw)}개')\n",
    "\n",
    "# 인칭 대명사\n",
    "# ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
    "for idx, _ in enumerate(english_sw[:100]):\n",
    "    print(_, end=' ' if idx%10 else '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토근에서 불용어 제거하기\n",
    "text = \"Family!!! is...! not&&&*& an... important thing. It's everything.....~~~~~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuation -> ! : Family is... not&&&*& an... important thing. It's everything.....~~~~~\n",
      "punctuation -> \" : Family is... not&&&*& an... important thing. It's everything.....~~~~~\n",
      "punctuation -> # : Family is... not&&&*& an... important thing. It's everything.....~~~~~\n",
      "punctuation -> $ : Family is... not&&&*& an... important thing. It's everything.....~~~~~\n",
      "punctuation -> % : Family is... not&&&*& an... important thing. It's everything.....~~~~~\n",
      "punctuation -> & : Family is... not* an... important thing. It's everything.....~~~~~\n",
      "punctuation -> ' : Family is... not* an... important thing. Its everything.....~~~~~\n",
      "punctuation -> ( : Family is... not* an... important thing. Its everything.....~~~~~\n",
      "punctuation -> ) : Family is... not* an... important thing. Its everything.....~~~~~\n",
      "punctuation -> * : Family is... not an... important thing. Its everything.....~~~~~\n",
      "punctuation -> + : Family is... not an... important thing. Its everything.....~~~~~\n",
      "punctuation -> , : Family is... not an... important thing. Its everything.....~~~~~\n",
      "punctuation -> - : Family is... not an... important thing. Its everything.....~~~~~\n",
      "punctuation -> . : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> / : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> : : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> ; : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> < : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> = : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> > : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> ? : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> @ : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> [ : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> \\ : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> ] : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> ^ : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> _ : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> ` : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> { : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> | : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> } : Family is not an important thing Its everything~~~~~\n",
      "punctuation -> ~ : Family is not an important thing Its everything\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Family is not an important thing Its everything'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구두점 제거\n",
    "import string \n",
    "\n",
    "punct = string.punctuation\n",
    "\n",
    "for pun in punct:\n",
    "    \n",
    "    text = text.replace(pun,'')\n",
    "    print(f'punctuation -> {pun} : {text}')\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['family', 'is', 'not', 'an', 'important', 'thing', 'its', 'everything']\n"
     ]
    }
   ],
   "source": [
    "# 대소문자 통일\n",
    "text = text.lower()\n",
    "\n",
    "# 토큰화 진행\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 : 분석에 의미가 없는 토큰 제거 \n",
    "for token in tokens:\n",
    "    if token in english_sw:\n",
    "        # 제거\n",
    "        tokens.remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['family', 'not', 'important', 'thing', 'everything']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'not',\n",
       " 'not']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not을 불용어로 빼고 싶을 때 english_sw list에 추가\n",
    "english_sw.append('not')\n",
    "english_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "    if token in english_sw:\n",
    "        # 제거\n",
    "        tokens.remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['family', 'important', 'thing', 'everything']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not 빠진 거 확인 \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was wondering if anyone out there could enlighten me on this car . "
     ]
    }
   ],
   "source": [
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "\n",
    "# 소문자로 만들어주기\n",
    "text = text.lower()\n",
    "\n",
    "# 토큰화 해주기\n",
    "text = word_tokenize(text)\n",
    "\n",
    "# 확인\n",
    "for _ in text: print(_, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동시제거 - 미리 제거 안해주고 토큰화 후에 구두점 제거해주는 방식 \n",
    "for token in tokens:\n",
    "    \n",
    "    # 불용어 제거 \n",
    "    if token in english_sw:\n",
    "        tokens.remove(token)\n",
    "    \n",
    "    # 구두점 제거 \n",
    "    if token in list(punct):\n",
    "        tokens.remove(punct)\n",
    "        \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_CV_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
