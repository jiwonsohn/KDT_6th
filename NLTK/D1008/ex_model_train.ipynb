{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 분류 모델\n",
    "- p.324 - p.330\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_vocab, hidden_dim,\t\t\t# n_vocab=> 단어사전 최대 길이\n",
    "            embedding_dim,\n",
    "            n_layers,\n",
    "            dropout=0,\n",
    "            bidirectinal=True,\n",
    "            model_type=\"lstm\"\n",
    "\t):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "               num_embeddings=n_vocab,\n",
    "               embedding_dim=embedding_dim,\n",
    "               padding_idx=0\n",
    "\t\t)\n",
    "        \n",
    "        if model_type == \"rnn\":\n",
    "            self.model == nn.RNN(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size= hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectinal,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "\t\t\t)\n",
    "            \n",
    "        elif model_type==\"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "\t\t\t\tinput_size = embedding_dim,\n",
    "                hidden_size= hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectinal,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "\t\t\t)\n",
    "        \n",
    "\t\t# 양방향학습 True\n",
    "        if bidirectinal:\n",
    "            self.classifier = nn.Linear(hidden_dim*2, 1)\n",
    "             \n",
    "        else:  \n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _  = self.model(embeddings)\n",
    "             \n",
    "        last_output = output[:,-1,:]\t\t\t\t# linear에 넣기 위한 Flatten\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# 마지막 시점만 결과만 분리해 분류기 계층 전달 \n",
    "        last_output = self.dropout(last_output)\n",
    "        \n",
    "        logits = self.classifier(last_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 불러오기-------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-43\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-43\\Korpora\\nsmc\\ratings_test.txt\n",
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size: 45000\n",
      "Testing Data Size: 5000\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load(\"nsmc\")\n",
    "corpusDF = pd.DataFrame(corpus.test)\n",
    "\n",
    "train = corpusDF.sample(frac=0.9, random_state=42)\n",
    "test = corpusDF.drop(train.index)\n",
    "\n",
    "print(train.head(5).to_markdown())\n",
    "print(f\"Training Data Size: {len(train)}\")\n",
    "print(f\"Testing Data Size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "# 데이터 토큰화 및 단어 사전 구축\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "\tcounter = Counter()\n",
    "    \n",
    "\tfor tokens in corpus:\n",
    "\t\tcounter.update(tokens)\n",
    "\t\n",
    "\tvocab = special_tokens\n",
    "\n",
    "\tfor token, count in counter.most_common(n_vocab):\n",
    "\t\tvocab.append(token)\n",
    "\n",
    "\treturn vocab\n",
    "\n",
    "tokenizer = Okt()\n",
    "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\",\"<unk>\"])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "it_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩 & 패딩\n",
    "import numpy as np\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    \n",
    "\tresult = list()\n",
    "\n",
    "\tfor seq in sequences:\n",
    "\t\tseq = seq[:max_length]\n",
    "\t\tpad_length = max_length - len(seq)\n",
    "\n",
    "\t\tpadded_sequnce = seq + [pad_value]*pad_length\n",
    "\n",
    "\t\tresult.append(padded_sequnce)\n",
    "\n",
    "\treturn np.asarray(result)\n",
    "\n",
    "unk_id = token_to_id[\"<unk>\"]\n",
    "\n",
    "train_ids = [\n",
    "\t[token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
    "]\n",
    "\n",
    "test_ids = [\n",
    "\t[token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
    "]\n",
    "\n",
    "max_length = 32\n",
    "pad_id = token_to_id[\"<pad>\"]\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 적용\n",
    "import torch \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n",
    "\n",
    "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
    "\n",
    "train_DS = TensorDataset(train_ids, train_labels)\n",
    "test_DS = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_DS, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_DS, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 & 최적화 함수 정의\n",
    "from torch import optim\n",
    "\n",
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimzier = optim.RMSprop(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 및 테스트\n",
    "def train(model, datasets, criterion, optimzier, device, interval):\n",
    "\tmodel.train()\n",
    "\tloss_tot = []\t\t# list()\n",
    "    \n",
    "\tfor step, (input_ids, labels) in enumerate(datasets):\n",
    "\t\tinput_ids = input_ids.to(device)\n",
    "\t\tlabels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "\t\tlogits = model(input_ids)\n",
    "\n",
    "\t\tloss = criterion(logits, labels)\n",
    "\t\tloss_tot.append(loss.item())\n",
    "\n",
    "\t\toptimzier.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimzier.step()\n",
    "\n",
    "\t\tif step % interval ==0:\n",
    "\t\t\tprint(f'Train loss {step} : {np.mean(loss_tot)}')\n",
    "\n",
    "def test(model, datasets, criterion, device):\n",
    "\tmodel.eval()\n",
    "\tloss_tot = []\n",
    "\tscore = []\n",
    "\n",
    "\tfor step, (input_ids, labels) in enumerate(datasets):\n",
    "\t\tinput_ids = input_ids.to(device)\n",
    "\t\tlabels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "\t\tlogits = model(input_ids)\n",
    "\t\tloss = criterion(logits, labels)\n",
    "\n",
    "\t\tloss_tot.append(loss.item())\n",
    "\n",
    "\t\t# 로지스틱\n",
    "\t\tpre_y = torch.sigmoid(logits) > 0.5\n",
    "\n",
    "\t\tscore.extend(\n",
    "\t\t\ttorch.eq(pre_y, labels).cpu().tolist()\n",
    "\t\t)\n",
    "    \n",
    "\tprint(f'Val loss: {np.mean(loss_tot)}, Val Score: {np.mean(score)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== {0} ==============================\n",
      "Train loss 0 : 0.6931707859039307\n",
      "Train loss 500 : 0.6930916133279096\n",
      "Train loss 1000 : 0.6784207920928101\n",
      "Train loss 1500 : 0.6674440251756397\n",
      "Train loss 2000 : 0.657576319844886\n",
      "Train loss 2500 : 0.6419902373103799\n",
      "Val loss: 0.5271316828628698, Val Score: 0.7374\n",
      "======================================================================\n",
      "============================== {1} ==============================\n",
      "Train loss 0 : 0.4230509102344513\n",
      "Train loss 500 : 0.4979621397996853\n",
      "Train loss 1000 : 0.48467893042526283\n",
      "Train loss 1500 : 0.4711747458781662\n",
      "Train loss 2000 : 0.46252894317400806\n",
      "Train loss 2500 : 0.4562458096772897\n",
      "Val loss: 0.41602776827998816, Val Score: 0.8028\n",
      "======================================================================\n",
      "============================== {2} ==============================\n",
      "Train loss 0 : 0.19948750734329224\n",
      "Train loss 500 : 0.37600717329217526\n",
      "Train loss 1000 : 0.37281208869430804\n",
      "Train loss 1500 : 0.3734809644445827\n",
      "Train loss 2000 : 0.3712198728512133\n",
      "Train loss 2500 : 0.37051304665709534\n",
      "Val loss: 0.38876364851435913, Val Score: 0.8154\n",
      "======================================================================\n",
      "============================== {3} ==============================\n",
      "Train loss 0 : 0.3124428987503052\n",
      "Train loss 500 : 0.3067045787018454\n",
      "Train loss 1000 : 0.3098963143696616\n",
      "Train loss 1500 : 0.3110341376846509\n",
      "Train loss 2000 : 0.3141649665440368\n",
      "Train loss 2500 : 0.3171290825145309\n",
      "Val loss: 0.3795511186265717, Val Score: 0.8254\n",
      "======================================================================\n",
      "============================== {4} ==============================\n",
      "Train loss 0 : 0.23263423144817352\n",
      "Train loss 500 : 0.2604958193001157\n",
      "Train loss 1000 : 0.2686912180385092\n",
      "Train loss 1500 : 0.26573708917273037\n",
      "Train loss 2000 : 0.26940533265225713\n",
      "Train loss 2500 : 0.27179388657081177\n",
      "Val loss: 0.3901004612255401, Val Score: 0.8114\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"=\"*30,{epoch},\"=\"*30)\n",
    "    train(classifier, train_loader, criterion, optimzier, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)\n",
    "    print(\"=\"*70)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "을 [-7.2241910e-02 -1.2390603e+00  6.8603176e-01  1.5073179e-01\n",
      " -2.6981872e-01  1.6092555e-01  1.2608292e+00  4.9933285e-01\n",
      "  2.8274119e-01  6.3064897e-01  1.4297815e-01 -1.2178882e+00\n",
      "  1.5678551e+00  2.0204496e+00  1.3050552e-01 -3.5524830e-01\n",
      " -5.7229286e-01  7.1214777e-01  1.8999766e+00  2.0279989e-01\n",
      " -9.7408062e-03  3.7015226e-01 -1.2747694e+00  2.1945402e-01\n",
      "  1.0166867e-01  2.7313355e-01  2.3684933e+00  4.0055183e-01\n",
      " -3.9525253e-01  7.3077828e-01 -5.3721541e-01 -1.1170708e+00\n",
      "  2.7867234e-01 -1.6353542e-01 -6.9091338e-01 -6.8649596e-01\n",
      " -2.6942199e-01  1.4521776e+00 -1.5798107e+00  3.9709952e-02\n",
      "  1.3587626e+00 -1.1555054e+00 -2.8701308e-01 -1.5596823e+00\n",
      "  2.4744058e-01  3.6817703e-01 -2.4767101e-01 -5.5280006e-01\n",
      " -1.0111893e+00  9.9024512e-02 -2.3046599e-01 -4.0864196e-01\n",
      " -1.5590028e-01  4.5918244e-01 -3.4504193e-01  5.3708833e-01\n",
      " -1.4072609e+00  1.6119199e+00 -8.6443865e-01 -1.0234309e-01\n",
      " -3.8908565e-01 -3.2798180e-01 -1.5098411e+00  1.3166368e+00\n",
      " -6.8289988e-02  4.7574830e-01 -1.6133964e+00  1.5492272e+00\n",
      "  5.8672595e-01 -3.5457310e-01  1.1146393e+00 -8.0130363e-01\n",
      " -6.0157299e-01 -5.8013827e-01  4.2511758e-01 -1.3284812e+00\n",
      "  6.6834468e-01 -6.3680685e-01  5.3528696e-01 -1.9443330e+00\n",
      "  6.8403077e-01  8.1806935e-02 -1.2523147e+00  4.5978874e-01\n",
      " -4.2809302e-01  4.3614623e-01  7.1957803e-01 -8.3077085e-01\n",
      "  7.7838719e-01  3.5132514e-05 -2.2290894e-01 -7.4779361e-01\n",
      " -1.3274280e+00 -4.7313291e-01  6.1551112e-01 -1.1527630e+00\n",
      " -1.7286274e+00  6.0361528e-01  1.8144700e-03  1.3514283e+00\n",
      "  7.3031873e-01  1.8021172e+00 -6.1788982e-01 -7.6697940e-01\n",
      "  2.5665426e-01  1.9362915e-01 -1.7381014e+00  1.0150773e-01\n",
      " -1.3635142e+00  4.2429546e-01 -4.6090102e-03 -8.3447903e-02\n",
      " -3.8334274e-01  5.5540121e-01  7.9874212e-01  9.4715673e-01\n",
      " -1.7572986e-01  4.4863477e-01  1.4119452e+00  6.3066304e-01\n",
      " -1.9556774e-01  3.2428628e-01 -2.3713437e-01 -1.0058886e+00\n",
      " -6.1612433e-01 -9.3685865e-01  4.4988346e-01 -1.9451632e+00]\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델로부터 임베딩 추출\n",
    "token_to_embedding = dict()\n",
    "\n",
    "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = emb\n",
    "    \n",
    "token = vocab[10]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습된 word2vec 임베딩 값으로 모델 학습\n",
    "- p.333 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습된 모델로 임베딩 계층 초기화\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "word2vec = Word2Vec.load(\"../models/word2vec.model\")\n",
    "\n",
    "init_embeddings = np.zeros( (n_vocab, embedding_dim))\n",
    "\n",
    "for index, token in it_to_token.items():\n",
    "    if token not in [\"<pad>\",\"<unk>\"]:\n",
    "        init_embeddings[index] = word2vec.wv[token]\n",
    "        \n",
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습된 임베딩 계층 적용\n",
    "class SentenceClassifier_Pre(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_vocab, hidden_dim,\t\t\t# n_vocab=> 단어사전 최대 길이\n",
    "            embedding_dim,\n",
    "            n_layers,\n",
    "            dropout=0,\n",
    "            bidirectinal=True,\n",
    "            model_type=\"lstm\",\n",
    "            pretrained_embedding = None\n",
    "\t):\n",
    "        super().__init__()\n",
    "        \n",
    "        if pretrained_embedding is not None:\n",
    "            \n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
    "\t\t\t)\n",
    "\n",
    "        else:\n",
    "             \n",
    "            self.embedding = nn.Embedding(\n",
    "                   num_embeddings=n_vocab,\n",
    "                   embedding_dim=embedding_dim,\n",
    "                   padding_idx=0\n",
    "\t\t    )\n",
    "        \n",
    "        if model_type == \"rnn\":\n",
    "            self.model == nn.RNN(\n",
    "                input_size = embedding_dim,\n",
    "                hidden_size= hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectinal,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "\t\t\t)\n",
    "            \n",
    "        elif model_type==\"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "\t\t\t\tinput_size = embedding_dim,\n",
    "                hidden_size= hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectinal,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "\t\t\t)\n",
    "        \n",
    "\t\t# 양방향학습 True\n",
    "        if bidirectinal:\n",
    "            self.classifier = nn.Linear(hidden_dim*2, 1)\n",
    "             \n",
    "        else:  \n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _  = self.model(embeddings)\n",
    "             \n",
    "        last_output = output[:,-1,:]\t\t\t\t# linear에 넣기 위한 Flatten\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# 마지막 시점만 결과만 분리해 분류기 계층 전달 \n",
    "        last_output = self.dropout(last_output)\n",
    "        \n",
    "        logits = self.classifier(last_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== {0} ==============================\n",
      "Train loss 0 : 0.7004722356796265\n",
      "Train loss 500 : 0.6583871606462254\n",
      "Train loss 1000 : 0.6431397670037025\n",
      "Train loss 1500 : 0.6287249683896992\n",
      "Train loss 2000 : 0.6063951867660959\n",
      "Train loss 2500 : 0.5904365351668647\n",
      "Val loss: 0.49641471130017656, Val Score: 0.7616\n",
      "======================================================================\n",
      "============================== {1} ==============================\n",
      "Train loss 0 : 0.4719693064689636\n",
      "Train loss 500 : 0.5040486927874788\n",
      "Train loss 1000 : 0.49529849545105353\n",
      "Train loss 1500 : 0.4905013521856502\n",
      "Train loss 2000 : 0.486651747689016\n",
      "Train loss 2500 : 0.48180876585875737\n",
      "Val loss: 0.4493689206651986, Val Score: 0.786\n",
      "======================================================================\n",
      "============================== {2} ==============================\n",
      "Train loss 0 : 0.6565530300140381\n",
      "Train loss 500 : 0.4527547110816438\n",
      "Train loss 1000 : 0.45372317415791436\n",
      "Train loss 1500 : 0.4497395373538365\n",
      "Train loss 2000 : 0.44714319962135024\n",
      "Train loss 2500 : 0.44595810623823856\n",
      "Val loss: 0.4310065093227088, Val Score: 0.7996\n",
      "======================================================================\n",
      "============================== {3} ==============================\n",
      "Train loss 0 : 0.40444833040237427\n",
      "Train loss 500 : 0.4174927618956851\n",
      "Train loss 1000 : 0.4250700102000684\n",
      "Train loss 1500 : 0.42984033189341675\n",
      "Train loss 2000 : 0.430877105336169\n",
      "Train loss 2500 : 0.42916137671492094\n",
      "Val loss: 0.4291605310032543, Val Score: 0.7862\n",
      "======================================================================\n",
      "============================== {4} ==============================\n",
      "Train loss 0 : 0.32061880826950073\n",
      "Train loss 500 : 0.4164904184684068\n",
      "Train loss 1000 : 0.4208114560578134\n",
      "Train loss 1500 : 0.42201405502533135\n",
      "Train loss 2000 : 0.4204619220797269\n",
      "Train loss 2500 : 0.4204923297847952\n",
      "Val loss: 0.418473164922894, Val Score: 0.8078\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 사전 학습된 임베딩을 사용한 모델 학습\n",
    "classifier = SentenceClassifier_Pre(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim,\n",
    "    n_layers=n_layers, pretrained_embedding = init_embeddings\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimzier = optim.RMSprop(classifier.parameters(), lr=0.001)\n",
    "\n",
    "epochs =5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"=\"*30,{epoch},\"=\"*30)\n",
    "    train(classifier, train_loader, criterion, optimzier, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEXT_018_230_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
