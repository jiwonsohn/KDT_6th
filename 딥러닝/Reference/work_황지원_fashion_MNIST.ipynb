{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Fashion MNIST & IRIS ] 모델 구현\n",
    "\n",
    "-  커스텀 데이터셋 생성\n",
    "\n",
    "- 커스텀 모델 생성\n",
    "\n",
    "- 학습/테스트 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optima\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "from torchmetrics.classification import F1Score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = '../data/fashion-mnist_train.csv'\n",
    "TEST_FILE  = '../data/fashion-mnist_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice\n",
      "DEVICE: cpu\n",
      "torch ver: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# random_state: 33\n",
    "torch.manual_seed(33)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Notice\")\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "print(f\"torch ver: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User Defined Datasets Class\n",
    "---\n",
    "- class function: base on file path, datasets\n",
    "- parents class: torch.utils.data.Dataset\n",
    "- class name: FileDataSet\n",
    "- parameter: file_path\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- function skill: data type transform DataFrame\n",
    "- function name: cover_dataframe\n",
    "- parameter: file_path, header=0\n",
    "- function return: DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cover_dataframe(file_path,header=0):\n",
    "    ext = file_path.rsplit('.')[-1]\n",
    "    \n",
    "    if ext == 'csv':\n",
    "        return pd.read_csv(file_path, header=header)\n",
    "    elif ext == 'json':\n",
    "        return pd.read_json(file_path, header=header)\n",
    "    elif ext in ['xlsx', 'xls']:\n",
    "        return pd.read_excel(file_path, header=header)\n",
    "    else:\n",
    "        return pd.read_table(file_path, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainFileDataSet(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        super().__init__()\n",
    "        data_df = cover_dataframe(file_path)\n",
    "\n",
    "        self.feature_df = data_df[data_df.columns[1:]]\n",
    "        self.label_df = data_df[[data_df.columns[0]]]\n",
    "        # if self.label_df[self.label_df.columns[0]].dtype == 'object':\n",
    "        #     label_df = pd.get_dummies(self.label_df)\n",
    "        #     label_df = label_df.astype('int64')\n",
    "        #     self.label_df = label_df\n",
    "            \n",
    "        # random_state: 33\n",
    "        # stratify: self.label_df\n",
    "        # train : val = 7 : 3\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(self.feature_df, self.label_df, random_state=33, stratify=self.label_df)\n",
    "        # self.X_val = pd.get_dummies(self.X_val).astype('int64')\n",
    "        # self.y_val = pd.get_dummies(self.y_val).astype('int64')\n",
    "        \n",
    "        self.n_row = self.feature_df.shape[0]\n",
    "        self.features = self.feature_df.columns\n",
    "        self.n_features = self.feature_df.shape[1]\n",
    "        self.labels = data_df.iloc[:, 0].unique()\n",
    "        self.n_labels = len(self.labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_row\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        X_train = torch.FloatTensor(self.X_train.iloc[idx].values).to(DEVICE)\n",
    "        X_test = torch.FloatTensor(self.X_val.iloc[idx].values).to(DEVICE)\n",
    "        y_train = torch.FloatTensor(self.y_train.iloc[idx].values).to(DEVICE)\n",
    "        y_test = torch.FloatTensor(self.y_val.iloc[idx].values).to(DEVICE)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestFileDataSet(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        super().__init__()\n",
    "        data_df = cover_dataframe(file_path)\n",
    "\n",
    "        self.feature_df = data_df[data_df.columns[1:]]\n",
    "        self.label_df = data_df[[data_df.columns[0]]]\n",
    "        # if self.label_df[self.label_df.columns[0]].dtype == 'object':\n",
    "        #     label_df = pd.get_dummies(self.label_df)\n",
    "        #     label_df = label_df.astype('int64')\n",
    "        #     self.label_df = label_df\n",
    "        \n",
    "        self.n_row = self.feature_df.shape[0]\n",
    "        self.features = self.feature_df.columns\n",
    "        self.n_features = self.feature_df.shape[1]\n",
    "        self.labels = data_df.iloc[:, 0].unique()\n",
    "        self.n_labels = len(self.labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_row\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "            feature_ts = torch.FloatTensor(self.feature_df.iloc[idx].values).to(DEVICE)\n",
    "            label_ts = torch.FloatTensor(self.label_df.iloc[idx].values).to(DEVICE)\n",
    "            return feature_ts, label_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Instance\n",
    "---\n",
    "- ANN classification model\n",
    "    - input layer: input feature\n",
    "    - output layer: predict label\n",
    "    - hidden layer: dynamic? or fixed?\n",
    "- parents class: nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model structure\n",
    "---\n",
    "- learning method: supervised learninf, multi-classification\n",
    "- learning algorithm: 모델이 없나? linear\n",
    "\n",
    "    - input layer: input 1784, output 1200, activation function: ReLU\n",
    "    - hidden layer: input 1200, output 700, activation function: ReLU\n",
    "    - hidden layer: input 700, output 200, activation function: ReLU\n",
    "    - hidden layer: input 200, output 50, activation function: ReLU\n",
    "    - output layer: input 50, output 10, activation function: None (지원안함. nn.CrossEntropyLoss 알아서 계산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(1784, 1200)\n",
    "        self.hidden_layer_1 = nn.Linear(1200, 700)\n",
    "        self.hidden_layer_2 = nn.Linear(700, 200)\n",
    "        self.hidden_layer_3 = nn.Linear(200, 50)\n",
    "        self.output_layer = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.input_layer(x))\n",
    "        y = F.relu(self.hidden_layer_1(y))\n",
    "        y = F.relu(self.hidden_layer_2(y))\n",
    "        y = F.relu(self.hidden_layer_3(y))\n",
    "        y = self.output_layer(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- validation & test function\n",
    "---\n",
    "- function name: testing\n",
    "- parameter: model, x, y\n",
    "- function return: loss, score\n",
    "- `must not update weight & bais`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, X_ts, y_ts):\n",
    "    with torch.no_grad():\n",
    "        y_ts = y_ts.reshape(-1).long()\n",
    "        pred = model(X_ts)\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()(pred, y_ts)\n",
    "        \n",
    "        score = F1Score(task='multiclass', num_classes=10)(pred, y_ts)\n",
    "    \n",
    "    return loss, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model learning\n",
    "---\n",
    "- function name: training\n",
    "- parameter: model, epochs, lr, batch_size, dataset\n",
    "- function return: loss, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tarining(dataset, model, epochs, lr, batch):\n",
    "    train_val_loss = {'train':[], 'val':[]}\n",
    "    train_val_score = {'train':[], 'val':[]}\n",
    "    optmizer = optima.Adam(model.parameters(), lr=lr)\n",
    "    data_dl = DataLoader(dataset, batch_size=batch)\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        total_train_loss, total_train_score = 0, 0\n",
    "        total_val_loss, total_val_score = 0, 0\n",
    "        for X_train, X_val, y_train, y_val in data_dl:\n",
    "            batch_cnt = 60000 / batch\n",
    "            y_train = y_train.reshape(-1).long()\n",
    "            \n",
    "            pred = model(X_train)\n",
    "            \n",
    "            loss = nn.CrossEntropyLoss()(pred, y_train)\n",
    "            total_train_loss += loss\n",
    "            \n",
    "            score = F1Score(task='multiclass', num_classes=10)(pred, y_train)\n",
    "            total_train_score += score\n",
    "            \n",
    "            optmizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optmizer.step()\n",
    "            \n",
    "            val_loss, val_score = testing(model, X_val, y_val)\n",
    "            total_val_loss += val_loss\n",
    "            total_val_score += val_score\n",
    "        \n",
    "        loss_train = (total_train_loss/batch_cnt).item()\n",
    "        score_train = (total_train_score/batch_cnt).item()\n",
    "        loss_val = (total_val_loss/batch_cnt).item()\n",
    "        score_val = (total_val_score/batch_cnt).itrm()\n",
    "        \n",
    "        train_val_loss['train'].append(loss_train)\n",
    "        train_val_score['train'].append(score_train)\n",
    "        train_val_loss['val'].append(loss_val)\n",
    "        train_val_score['val'].append(score_val)\n",
    "        \n",
    "        print(f\"[{epoch:5}/{epochs:5}]  [Train]      loss: {loss_train:.6f}, score: {score_train:.6f}\")\n",
    "        print(f\"[{epoch:5}/{epochs:5}]  [Validation] loss: {loss_val:.6f}, score: {score_val:.6f}\")\n",
    "    \n",
    "    return train_val_loss, train_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TrainFileDataSet(TRAIN_FILE)\n",
    "test_ds = TestFileDataSet(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationModel(\n",
      "  (input_layer): Linear(in_features=1784, out_features=1200, bias=True)\n",
      "  (hidden_layer_1): Linear(in_features=1200, out_features=700, bias=True)\n",
      "  (hidden_layer_2): Linear(in_features=700, out_features=200, bias=True)\n",
      "  (hidden_layer_3): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (output_layer): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "ClassificationModel                      --\n",
       "├─Linear: 1-1                            2,142,000\n",
       "├─Linear: 1-2                            840,700\n",
       "├─Linear: 1-3                            140,200\n",
       "├─Linear: 1-4                            10,050\n",
       "├─Linear: 1-5                            510\n",
       "=================================================================\n",
       "Total params: 3,133,460\n",
       "Trainable params: 3,133,460\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model, end='\\n\\n')\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x784 and 1784x1200)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train_dl = DataLoader(train_ds, batch_size=32)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loss_dict, score_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtarining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[50], line 14\u001b[0m, in \u001b[0;36mtarining\u001b[1;34m(dataset, model, epochs, lr, batch)\u001b[0m\n\u001b[0;32m     11\u001b[0m batch_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60000\u001b[39m \u001b[38;5;241m/\u001b[39m batch\n\u001b[0;32m     12\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m---> 14\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(pred, y_train)\n\u001b[0;32m     17\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\torch_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\torch_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[48], line 12\u001b[0m, in \u001b[0;36mClassificationModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     y \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     13\u001b[0m     y \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer_1(y))\n\u001b[0;32m     14\u001b[0m     y \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer_2(y))\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\torch_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\torch_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\torch_38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x784 and 1784x1200)"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_dl = DataLoader(train_ds, batch_size=32)\n",
    "loss_dict, score_dict = tarining(train_ds, model, 1000, 0.001, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
